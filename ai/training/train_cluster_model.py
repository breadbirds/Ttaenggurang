import pickle
import numpy as np
import pandas as pd
import joblib
from sklearn.cluster import KMeans, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.metrics import silhouette_score, accuracy_score, classification_report, precision_score, recall_score, f1_score
import os
os.environ["LOKY_MAX_CPU_COUNT"] = "4"  # ì›í•˜ëŠ” ì½”ì–´ ê°œìˆ˜ë¡œ ì„¤ì •


# ğŸ“Œ ë°ì´í„° ìƒì„± (10ë§Œ ê°œ ìƒ˜í”Œ)
np.random.seed(42)
num_students = 100000

# ë°ì´í„° ìƒì„±
total_income = np.random.randint(1000000, 5000000, num_students)  # ì´ì†Œë“ (100ë§Œ ~ 500ë§Œ)

# **ë¹„ìœ¨ ê¸°ë°˜ì´ ì•„ë‹ˆë¼ ê°œë³„ì ìœ¼ë¡œ ëœë¤ê°’ì„ ìƒì„±í•˜ëŠ” ê¸°ì¡´ ë°©ì‹ ìœ ì§€**
savings = np.random.randint(10000, 3000000, num_students)  # ì €ì¶•ì•¡ (1ë§Œ ~ 300ë§Œ)
investment = np.random.randint(10000, 3000000, num_students)  # íˆ¬ìì•¡ (1ë§Œ ~ 300ë§Œ)
spending = np.random.randint(10000, 3000000, num_students)  # ì†Œë¹„ì•¡ (1ë§Œ ~ 300ë§Œ)
salary = np.random.randint(800000, 3000000, num_students)  # ê¸‰ì—¬ (80ë§Œ ~ 300ë§Œ)
incentive = np.random.randint(0, 500000, num_students)  # ì¸ì„¼í‹°ë¸Œ (0 ~ 50ë§Œ)

# **ì´í•©ì´ total_incomeì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì¡°ì •**
total_expense = savings + investment + spending
excess_mask = total_expense > total_income

# ì´ˆê³¼í•˜ëŠ” ê²½ìš°, ë¹„ìœ¨ì„ ì¬ì¡°ì •í•˜ì—¬ total_incomeì„ ë„˜ì§€ ì•Šê²Œ ë§Œë“¦
savings[excess_mask] = (savings[excess_mask] / total_expense[excess_mask]) * total_income[excess_mask]
investment[excess_mask] = (investment[excess_mask] / total_expense[excess_mask]) * total_income[excess_mask]
spending[excess_mask] = (spending[excess_mask] / total_expense[excess_mask]) * total_income[excess_mask]

# ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = pd.DataFrame({
    'total_income': total_income,
    'salary': salary,
    'incentive': incentive,
    'savings': savings.astype(int),
    'investment': investment.astype(int),
    'spending': spending.astype(int)
})

# ğŸ“Œ 6ï¸âƒ£ ì£¼ìš” ë¹„ìœ¨ ê³„ì‚° (ìŠ¤ì¼€ì¼ë§ ì—†ì´ ì‚¬ìš©)
data['savings_ratio'] = data['savings'] / data['total_income']
data['investment_ratio'] = data['investment'] / data['total_income']
data['spending_ratio'] = data['spending'] / data['total_income']

# ì •ê·œí™” ì ìš©
features = ['savings_ratio', 'investment_ratio', 'spending_ratio']
scaler = MinMaxScaler()
# scaler = StandardScaler()
# scaler = RobustScaler()
X_scaled = scaler.fit_transform(data[features])

# ğŸ“Œ 3ï¸âƒ£ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ í•™ìŠµ
kmeans = KMeans(n_clusters=3, random_state=42)
data['kmeans_cluster'] = kmeans.fit_predict(X_scaled)

dbscan = DBSCAN(eps=0.1, min_samples=5)
data['dbscan_cluster'] = dbscan.fit_predict(X_scaled)

gmm = GaussianMixture(n_components=4, random_state=42, covariance_type='diag')
data['gmm_cluster'] = gmm.fit_predict(X_scaled)

# ğŸ“Œ 4ï¸âƒ£ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (Silhouette Score ê¸°ì¤€)
kmeans_silhouette = silhouette_score(X_scaled, data['kmeans_cluster'])
dbscan_silhouette = silhouette_score(X_scaled, data['dbscan_cluster']) if len(set(data['dbscan_cluster'])) > 1 else -1
gmm_silhouette = silhouette_score(X_scaled, data['gmm_cluster'])
print(kmeans_silhouette, dbscan_silhouette, gmm_silhouette)

# ğŸ“Œ ìµœì ì˜ ëª¨ë¸ ì„ íƒ (best_model)
best_model, best_labels = max([
    (kmeans, data['kmeans_cluster'], kmeans_silhouette, "K-Means"),
    (dbscan, data['dbscan_cluster'], dbscan_silhouette, "DBSCAN"),
    (gmm, data['gmm_cluster'], gmm_silhouette, "GMM")
], key=lambda x: x[2])[:2]  # ì‹¤ë£¨ì—£ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ëª¨ë¸ ì„ íƒ

print(f"ğŸ¯ ìµœì ì˜ ëª¨ë¸ì€: {best_model.__class__.__name__}")

data['cluster'] = best_labels  # ìµœì  ëª¨ë¸ì˜ í´ëŸ¬ìŠ¤í„° ì»¬ëŸ¼ì„ 'cluster'ë¡œ ì €ì¥

# ğŸ“Œ 5ï¸âƒ£ í´ëŸ¬ìŠ¤í„° ë§¤í•‘ (ì†Œë¹„í˜•, ì €ì¶•í˜•, íˆ¬ìí˜•)
cluster_means = data.groupby(best_labels)[features].mean()
cluster_mapping = {
    cluster_means['spending_ratio'].idxmax(): 'ì†Œë¹„í˜•',
    cluster_means['savings_ratio'].idxmax(): 'ì €ì¶•í˜•',
    cluster_means['investment_ratio'].idxmax(): 'íˆ¬ìí˜•'
}
print("ğŸ“Œ í´ëŸ¬ìŠ¤í„° ë§¤í•‘:", cluster_mapping)

# ğŸ“Œ 6ï¸âƒ£ ëª¨ë¸ ë° í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ì €ì¥ (pkl íŒŒì¼)
model_data = {
    "model": best_model,
    "cluster_mapping": cluster_mapping
}

# ğŸ“Œ í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶œë ¥
print("âœ… ìµœì  ëª¨ë¸:", best_model.__class__.__name__)
print("ğŸ“Œ ì‹¤ë£¨ì—£ ì ìˆ˜:", max(kmeans_silhouette, dbscan_silhouette, gmm_silhouette))
print("ğŸ“Œ ìµœì¢… í´ëŸ¬ìŠ¤í„° ê°œìˆ˜:", len(set(best_labels)))
print("ğŸ“Œ í´ëŸ¬ìŠ¤í„° ë¶„í¬:")
print(data['cluster'].value_counts())
print("ğŸ“Œ í´ëŸ¬ìŠ¤í„°ë³„ í‰ê· ê°’:")
print(cluster_means)

# ----------------------------------------------

# ğŸ“Œ ì§€ë„ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
X = data[['savings_ratio', 'spending_ratio', 'investment_ratio']]
y = best_labels

# ğŸ“Œ ë¼ë²¨ ì¸ì½”ë”©
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# ğŸ“Œ ë°ì´í„° ì˜¤ë²„ìƒ˜í”Œë§ (SMOTE)
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y_encoded)

# ğŸ“Œ ì§€ë„ í•™ìŠµ - ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸° í•™ìŠµ
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)
rf_model = RandomForestClassifier(n_estimators=150, max_depth=7, random_state=42)
rf_model.fit(X_train, y_train)

# ğŸ“Œ ì§€ë„ í•™ìŠµ ì„±ëŠ¥ í‰ê°€
y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
print("âœ… ì§€ë„ í•™ìŠµ ì •í™•ë„:", accuracy)
print("ğŸ“Œ ì •ë°€ë„(Precision):", precision)
print("ğŸ“Œ ì¬í˜„ìœ¨(Recall):", recall)
print("ğŸ“Œ F1 ì ìˆ˜:", f1)
print("ğŸ“Œ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, y_pred))


# ğŸ“Œ ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
model_dir = "./model"
os.makedirs(model_dir, exist_ok=True)

# ğŸ“Œ ëª¨ë¸ ì €ì¥
joblib.dump(best_model, f"{model_dir}/cluster_model.pkl")  # ë¹„ì§€ë„ í•™ìŠµ ëª¨ë¸
joblib.dump(scaler, f"{model_dir}/scaler.pkl")  # ì •ê·œí™” ë„êµ¬
joblib.dump(cluster_mapping, f"{model_dir}/cluster_mapping.pkl")  # í´ëŸ¬ìŠ¤í„° ë§¤í•‘
joblib.dump(rf_model, f"{model_dir}/rf_model.pkl")  # ì§€ë„ í•™ìŠµ ëª¨ë¸
joblib.dump(label_encoder, f"{model_dir}/label_encoder.pkl")  # ë¼ë²¨ ì¸ì½”ë”

print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: cluster_model.pkl, scaler.pkl, cluster_mapping.pkl, rf_model.pkl, label_encoder.pkl")
